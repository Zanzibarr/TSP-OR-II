Up to this point we have explored two different approaches to solving the TSP problem that can be considered the extremes of a scale. On one hand we have several heuristic algorithms that can quickly solve instances with little guarantee of finding an optimal solution; on the other hand we have exact algorithms based on cplex, which can find optimal solutions for small instances.

To solve mid-sized instances (around 1,000 nodes) we use a series of hybrid methods called \textit{matheuristics} \cite{Fischetti2016}. These methods take a closed-box MIP solver, that is guaranteed to find an optimal solution to the model they receive in input, and we use it as a heuristic; this is achieved by giving as input a restricted version of the original model, from which an arbitrary set of feasible solutions has been excluded. In this way, we still exploit the power of the MIP solver, while restricting the space of possible solutions, thus reducing the required time to solve the model.

\section{Diving}

This matheuristic is based on iteratively solving the TSP model and restricting it by taking the best solution found up to a certain iteration and \textit{hard fixing} some of its edges, using a heuristic solution as the starting incumbent. These edges are called \textit{'yes' edges} and they are fixed by setting the values of their respective variables in the model to 1. This method is called \textit{diving} because fixing a series of variables is analogous to reaching a certain depth of the branch \& bound tree in a single iteration.

There are several possible approaches to decide how many and which edges to fix. In this thesis we choose them in a completely random way. While it is the simplest possible approach, it has the advantage of having a very small probability of getting stuck on a certain neighborhood of solutions.

\newpage

\subsection{Pseudocode}
\begin{algorithm}[h]
    \caption{Diving matheuristic algorithm}
    \textbf{Input} $pfix$ edge fixing probability\\
    \textbf{Output} Hamiltonian cycle of V, cost of cycle\\
    \begin{algorithmic}

        \State $x^H \gets$ *heuristic solution of TSP on G*
        \State *build CPLEX model from $G$ with objective function and degree constraints*\\
        \While{*time limit not exceeded*}
        \State $\tilde{E} \gets$ *subset of $E$ with $|V|*pfix$ edges of $x^H$ chosen at random with probability $pfix$*
        \State *fix every $x_e^H\in\tilde{E}$ to $1$ in model*\\
        \State *$x^*\gets$ solution returned by CPLEX for input model with fixed edges*
        \If{$\text{cost}(x^*)\leq \text{cost}(x^H)$}
        \State $x^H\gets x^*$
        \EndIf\\
        \State *unfix every $x_e^H\in\tilde{E}$ in model*
        \EndWhile

    \end{algorithmic}
\end{algorithm}
\FloatBarrier

\subsection{Hyperparameter tuning}

$pfix$ is a hyperparameter of the algorithm, thus we tested different values for it.

\begin{figure}[h]
    \centering
    \includegraphics*[width=.6\textwidth]{../plots/perfprof_hard_costs.png}
    \caption*{20 instances, 1000 nodes, time limit: 120s}
\end{figure}
\FloatBarrier

As we can see, the results obtained by the diving algorithm does not improve with respect to the ones provided by our best CPLEX model: this might be due to the fact that by hard fixing some edges, much of the work our model does to patch solutions inside the callbacks is lost, since by patching we might generate solutions which would be rejected by the model with the fixed variables.

As we saw, the major performance boost we had in our CPLEX model, came from patching and posting solutions inside the callbacks and without it, hardfixing has a big performance decrease.

\section{Local Branching}

In the diving algorithm, we decided both how many and which variables to fix in the mathematical problem before using the cplex solver. In the \textit{local branching} algorithm \cite{Fischetti2003}, instead, we choose only how many variables we want to fix, leaving to the TSP solver the responsibility of choosing which ones to fix. This is expressed in the model by adding an additional constraint.

Given $x^H$ the current incumbent, we define the \textit{local branching constraint} as:
$$\sum_{e:x^H_e=1}x_e\geq n-k$$
The left-hand sum is the number of variables we want the TSP solver to keep from $x^H$, while $k$ is the number of variables we want the TSP solver to fix. By setting the direction of the constraint as $\geq$, the solver explores a neighborhood of different solutions that can be reached by changing $n-k$ variables (looking for the best k-opt swap to apply).

This constraint is not guaranteed to be valid for the set of feasible solution, because it might cut out the optimal solution; however, it allows us to greatly reduce the integrality gap.

\FloatBarrier
\subsection{Pseudocode}
\begin{algorithm}[h]
    \caption{Local branching matheuristic algorithm (v1)}
    \textbf{Input} integer $k_{\text{init}}$\\
    \textbf{Output} Hamiltonian cycle of V, cost of cycle\\
    \begin{algorithmic}

        \State $x^H \gets$ *heuristic solution of TSP on G*
        \State *build CPLEX model from $G$ with objective function and degree constraints*
        \State $k\gets k_{\text{init}}$
        \While{*time limit not exceeded*}
        \State *add constraint local branching constraint $\sum_{e:x_e^H=1}x_e\geq n-k$*
        \State $x^*\gets$ *solution returned by CPLEX for input model with local branching constraint*
        \If{$\text{cost}(x^*)\leq \text{cost}(x^H)$}
        \State $x^H\gets x^*$
        \EndIf
        \If{*$x^H$ has not been improved for 5 iterations*}
        \State $k\gets k+10$
        \EndIf
        \State *save improvements*
        \State *remove local branching constraint*
        \EndWhile\\\\

        \Return $c^H$, *cost of $x^H$*

    \end{algorithmic}
\end{algorithm}
\FloatBarrier

\subsection{Hyperparameter tuning}

In this algorithm, $k_{\text{init}}$ is the hyperparameter.

\begin{figure}[h]
    \centering
    \includegraphics*[width=.6\textwidth]{../plots/perfprof_lbv1_costs.png}
    \caption*{20 instances, 1000 nodes, time limit: 120s}
\end{figure}

Similarly to the diving algorithm, the gap between the costs found by out best setting of CPLEX and the local branching algorithm is smaller than 10\%. We can also see an improvement for greater starting values of $k$.

\subsection{A more dynamic approach}
The choice of $k$ helps us reducing the integrality gap, so we might want to change this value dynamically based on the results of each iteration of the local branching algorithm.

As we can see, with this approach the algorithm manages to move through the search space more smoothly, improving its performances of a 6-8\% factor, beating our best CPLEX model.

\begin{figure}[h]
    \centering
    \includegraphics*[width=.6\textwidth]{../plots/perfprof_lb_costs.png}
    \caption*{20 instances, 1000 nodes, time limit: 120s}
\end{figure}

\begin{algorithm}[h]
    \caption{Local branching matheuristic algorithm (v2)}
    \textbf{Input} integer $k_{\text{init}} = 100$\\
    \textbf{Output} Hamiltonian cycle of V, cost of cycle\\
    \begin{algorithmic}

        \State $x^H \gets$ *heuristic solution of TSP on G*
        \State *build CPLEX model from $G$ with objective function and degree constraints*
        \State $k\gets k_{\text{init}}$

        \While{*time limit not exceeded*}

            \If{*this is a new iteration*}
                \State *add constraint local branching constraint $\sum_{e:x_e^H=1}x_e\geq n-k$*
                \State *add the best solution found so far as a warm start*
            \EndIf

            \State $x^*\gets$ *solution returned by CPLEX for input model with local branching constraint*

            \If{*CPLEX exited for time limit and no visible improvement*}

                \If{*more than 3 repetitions*}
                    \State $k\gets k+10$
                    \State *finish this iteration*
                \Else
                    \State *repeat this iteration without changing CPLEXs model*
                \EndIf
            \Else
                \If{*CPLEX found optimal solution for this model*}
                    \State $k\gets k-10$
                \EndIf
            \EndIf

            \State *save improvements*
            \State *remove local branching constraint*

        \EndWhile\\\\

        \Return $c^H$, *cost of $x^H$*

    \end{algorithmic}
\end{algorithm}

\newpage

\subsection{Considering multiple solutions}

The idea behind local branching is to limit the search space using a solution that we know is feasible, but not optimal. The constraint we add to the model ensures that we only fix how many edges we want to take from the solution found in the past iteration. If we want to consider more than one past solution, we can use this variation of the constraint:
$$\sum_{e\in E}\mbox{count}(e)\cdot x_e\geq n-k$$
where count($e$) is the number of times edge $e$ has been considered in past solutions. Note that if we consider just the solution from the last iteration of local branching, count($e$)$\in\{0,1\} \ \forall e\in E$, which leads to the first version of the constraint.

With this constraint we give more importance to edges that have been considered "optimal" in more than one solution. We follow the hypotesis that edges that are found more times are more likely to be in the optimal solution.

The mathematical meaning of the constraint is changed: in the normal local branching, $k$ is the exact number of edges that CPLEX can change, while in this constraint this claim does not hold. The left-hand side of the constraint now gives more importance to edges whose count is greater than 1; if CPLEX chooses to fix an edge with an high count, it has a bigger search space, since count($e$) will bring the lower bound $n-k$ closer, hence it now has more freedom.

To sum up, with this constraint CPLEX will still search the best k-opt swap around the suggested solution, but it will also try solutions with more swaps around solutions whose edges have been already considered in other iterations of the local branching.

\subsection{Results analysis}

The shown constraint is not optimal: if the number of solutions considered start to increase indefinitely the left-hand side of the constraint will reach large values, so large that the meaning of the degrees of freedom that we give to CPLEX, $k$, will start to vanish.

In our implementation this is not a problem since we give each iteration of local branching 1/10 of the total time limit, so at most $\text{count}(e)\leq10$. Moreover, with the v2 version of the algorithm illustrated in section 6.2.3 we almost never reach such a high count.

To avoid this problem, we made two different versions of this local branching: one that considers each of the past solutions found, and one which considers only a fixed number of recent past solutions.

\begin{figure}[h]
    \centering
    \includegraphics*[width=.6\textwidth]{../plots/perfprof_lbv2_costs.png}
    \caption*{20 instances, 1000 nodes, time limit: 120s}
\end{figure}

Here we can see that using the new constraint gives slightly better solutions than the original one and that limiting the number of past solutions considered in the constraint does not make a difference, as previously explained.

\section{Comparison diving / local branching}

\begin{figure}[h]
    \centering
    \includegraphics*[width=.6\textwidth]{../plots/perfprof_mat_costs.png}
    \caption*{20 instances, 1000 nodes, time limit: 120s}
\end{figure}

Beating our best CPLEX model was hard given the range of optimization techniques we used, but in the end we managed to find a matheuristic method which can help us with problems which are too big for CPLEX to handle by itself.