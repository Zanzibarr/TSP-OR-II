A \textit{metaheuristic} is an abstact approach that can be applied to a range of problems, which can then be specialized. In our case, our metaheuristic algorithms use more than one heuristic in order to guide the search processs to efficiently explore the search space.

We explore two metaheuristic algorithm, which both unlock a greater search space compared to the aforementioned heuristic methods by allowing "bad moves" to escape a locally optimal solution.

\section{Tabu Search}
The \textit{tabu search} algorithm \cite{Glover1990} is based on the idea of allowing the 2-opt algorithm to perform swaps that still are the best ones, but not necessarily swaps that improve the cost of the solution. Once we find a local optima, the tabu search algorithm will keep searching, moving away from that locally optimal solution hoping to find a new one with a lower cost, whereas the 2-opt algorithm would stop.

If we allowed a bad move, at the next iteration the best move found by the 2-opt procedure would revert it, since that would be the only swap that lowers the cost. To prevent this, we need to keep track of those bad moves and prevent them from being reverted, marking them as \textit{tabu moves}, hence the name of the algorithm.

\subsection{Storing a tabu move}

A tabu move is intended as the worsening move that has been done in a previous round, and it basically consists on the 2 edges, or equivalently the 4 nodes, that were considered in the swap.

To store the tabu move we have more options on what to mark as a tabu move:

\begin{enumerate}
    \item one of the nodes (fix the two edges connected to that node)
    \item both nodes (fix all four edges in the swap)
    \item one or more edges
\end{enumerate}

Marking one or both nodes as tabu moves would restrict our area of search, since after a few tabu moves, lots of edges cannot be changed, so we opted to mark as a tabu move the two edges $(p_{i},p_{i+1})$ and $(p_{j}, p_{j+1})$.

\subsection{The tabu list}

The tabu list is intended as the list of tabu moves that 2-opt will need to consult to see whether a move is admitted or not. Once the tabu list is filled up, the oldest tabu move will be removed to let the one to be saved.

An important parameter of the tabu list is its size: a small size means that the algorithm is not very free to explore the search space, while a big size means that the algorithm will worsen the solution too much, possibly preventing it to ever find a better solution. This can also be intepreted as setting its memory: a small tabu list will forget earlier tabu moves, while a big tabu list will have a longer memory.

The size (or memory) of the tabu list will hereby be referred as the its \textit{tenure}. We built the tabu list as a fixed length array, where we stored each tabu move together with a counter which increases at each new tabu move. We used the counter to see if a move is still a tabu move or not: by comparing the current counter with the one stored along the move, we can check how many iterations has passed and if more iterations than the tenure has occurred, that moves is no longer a tabu move.

We tried out two ways of checking the tenure:

\begin{enumerate}
    \item \textit{static approach}: a move in the list is no longer a tabu move if
    $$\text{counter}-\text{counter(move)} < \text{tenure}$$
    where $\text{counter(move)}$ is the counter stored along the move in the tabu list.
    \item \textit{dynamic approach}: a move in the list is no longer a tabu move if
    $$\text{counter}-\text{counter(move)} < f(\text{tenure}, \text{counter})$$
    where $f(\text{tenure},\text{counter})\coloneq A\cdot\sin(\text{counter}\cdot B) + \text{tenure}$, and $A$, $B$ are parameters specified by the user (A will be referred as variable\_tenure).
\end{enumerate}

The dinamic approach performs better for a few reasons:

\begin{enumerate}
    \item it lowers the risk of remaining stuck for many iterations: when we reach a local optimum, with the dynamic approach the algorithm will start to forget some moves in a few iterations and will escape from that situation;
    \item it allows for a more dynamic exploration of the search space: the dynamic approach allows the algorithm to "forget" something to look for a better solution in the search space, but then remember it later if that leads to nothing.
\end{enumerate}

\subsection{Pseudocode}

\begin{algorithm}
    \caption{TSP tabu search algorithm}
    
    \textbf{Input} starting node $s\in V$\\
    \textbf{Output} Hamiltonian cycle of V, cost of cycle\\
    \begin{algorithmic}
        
        \State (cycle, cost) $\gets$ *result of greedy algorithm on $s$ and $V$*\\

        \While{*time limit not exceeded*}
            \State $(i, j)\gets$ *swap to be applied on cycle (checking the tabu list)*
            \If{*swap (i,j) leads to a worse position*}
                \State *add $(i,j)$ to tabu list*
            \EndIf
            \State *update cost*
            \State *reverse section of cycle between indices $i+1$ and $j$*

        \EndWhile\\\\

        \Return cycle, cost
    \end{algorithmic}
\end{algorithm}

Where the $\mbox{find\_tabu\_swap()}$ method returns the best swap (allowing for bad moves) after checking the tabu list.

\subsection{Results analysis}

\begin{figure}[h]
    \centering
    \includegraphics*[width=.6\textwidth]{../plots/perfprof_tabu_costs.png}
    \caption*{20 instances, 600 nodes, time limit: 120s}
\end{figure}
\FloatBarrier

In this plot are shown various versions of the tabu algorithm, with different parameters to try find good parameters: the first two tabu search algorithms use a tenure of 1/10 and 1/5 of the number of nodes while using the static tenure approach; the last two use the dynamic approach insthead, whith a variable\_tenure of 1/10 and 1/5 of the number of nodes.

As we expected, the dynamic approach yields slightly better results than the static approach. However, it is important to point out that the latter generates worse solutions than the 2-opt algorithm: this is due to the fact that the 2-opt algorithm looks among all possible greedy solutions based on the starting node, while the tabu algorithm only uses a fixed number of them to explore the search space.

We can see how the dynamic approach manages to surpass this limitation and improve the solutions with respect to the 2-opt algorithm.

\section{Variable Neighborhood Search (VNS)}

For the tabu algorithm to work, a list of moves must be stored as tabu moves: how many moves to store? Which one works better: a static or a dynamic tenure? And in the latter, how much should the tenure vary? This set of hyperparameters should be set with procedures that are unaffordable for the scope of this thesis. We may also run into overfitting.

A metaheuristic method that does not require hyperparameters is the \textit{Variable Neighborhood Search (VNS)} \cite{Hansen2019}, which approaches the same base idea of tabu search in a different way. Once we are in a local minimum, if we make a 2-opt swap (as we do with the tabu search algorithm), we must save that move as a tabu move, since the next 2-opt swap will revert it. The VNS approach is to make a swap that requires more than two edges to be swapped (entering the family of k-opt), in our case a 3-opt swap. Once a 3-opt swap is performed, it is impossible for a 2-opt swap to reverse that change, since it should change 3 edges and is allowed to change only 2 of them.

In some scenarios one 3-opt swap is enough to escape the local minimum, but in other it is not and more swaps are needed, thus creating a hyperparameter. To prevent this, we used multithreading to perform different numbers of 3-opt swaps on a local minimum, then use the 2-opt algorithm to lower the cost, and choose the best among the solutions found.

Another approach that can be explored in the future is using multithreading to keep the $k$ best choices among the solutions found and keep exploring them in parallel, with special attention to keep the list of parallel runs under control, avoiding exponential growth.

\newpage

\subsection{Pseudocode}

\begin{algorithm}
    \caption{TSP VNS algorithm}
    
    \textbf{Input} starting node $s\in V$\\
    \textbf{Output} Hamiltonian cycle of V, cost of cycle\\
    \begin{algorithmic}
        
        \State (cycle, cost) $\gets$ *result of greedy algorithm on $s$ and $V$*\\

        \While{*time limit not exceeded*}\\

            \State (cycle, cost) $\gets$ *apply random kicks to the cycle*
            \State (cycle, cost) $\gets$ *apply 2-opt to the cycle*\\

        \EndWhile\\\\

        \Return cycle, cost
    \end{algorithmic}
\end{algorithm}

The kicks we applied are 3-opt swaps using the following schema:

\begin{figure}[h]

    \centering
    \begin{subfigure}[c]{.4\textwidth}
        \centering
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw}]
                    \node (I) at (2.5,4) {$p_i$};
                    \node (II) at (0,3) {$p_{i+1}$};
                    \node (J) at (0,0) {$p_j$};
                    \node (JJ) at (2,-1) {$p_{j+1}$};
                    \node (K) at (4.5,0.3) {$p_k$};
                    \node (KK) at (4.5,2.7) {$p_{k+1}$};
                \end{scope}

                \begin{scope}[>={Stealth[black]}, every node/.style={fill=white,circle},
                            every edge/.style={draw=red,very thick}]
                    \path[->] (I) edge[draw=black] (II);
                    \path[->] (J) edge[draw=black] (JJ);
                    \path[->] (K) edge[draw=black] (KK);
                    \path[->] (II) edge[dashed, draw=black, thin, bend right=40] (J);
                    \path[->] (JJ) edge[dashed, draw=black, thin, bend right=40] (K);
                    \path[->] (KK) edge[dashed, draw=black, thin, bend right=40] (I);
                \end{scope}
            \end{tikzpicture}
        }
    \end{subfigure}
    \raisebox{-0.5\height}{$\Rightarrow$}
    \begin{subfigure}[c]{.4\textwidth}
        \centering
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
                \begin{scope}[every node/.style={circle,thick,draw}]
                    \node (I) at (2.5,4) {$p_i$};
                    \node (II) at (0,3) {$p_{i+1}$};
                    \node (J) at (0,0) {$p_j$};
                    \node (JJ) at (2,-1) {$p_{j+1}$};
                    \node (K) at (4.5,0.3) {$p_k$};
                    \node (KK) at (4.5,2.7) {$p_{k+1}$};
                \end{scope}

                \begin{scope}[>={Stealth[black]}, every node/.style={fill=white,circle},
                            every edge/.style={draw=red,very thick}]
                    \path[->] (I) edge[draw=blue] (K);
                    \path[->] (JJ) edge[draw=blue] (II);
                    \path[->] (J) edge[draw=blue] (KK);
                    \path[->] (II) edge[dashed, draw=black, thin, bend right=40] (J);
                    \path[->] (K) edge[dashed, draw=blue, thin, bend left=40] (JJ);
                    \path[->] (KK) edge[dashed, draw=black, thin, bend right=40] (I);
                \end{scope}
            \end{tikzpicture}
        }
    \end{subfigure}
    \caption*{$p_i\coloneq\mbox{cycle}[i]$}

\end{figure}

Note that, if we follow this schema, we need to reverse the cycle between $p_{j+1}$ and $p_k$.

\newpage

\section{Comparison Tabu Search / VNS}

\FloatBarrier
\begin{figure}[h]
    \centering
    \includegraphics*[width=.6\textwidth]{../plots/perfprof_met_costs_result.png}
    \caption*{20 instances, 600 nodes, time limit: 120s}
\end{figure}

As we can see, the VNS algorithm has consistently better performances than the tabu search algorith, up to a 3\% improvement: this might be due to the fact that the tabu search algorithm requires a finer parameter tuning, while the VNS algorithm does not need any.